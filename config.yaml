name: phonelm-100M
resume: False
profile: False
training:
    use_bf16: True
    context_size: 2048

    num_train_epochs: 5
    learning_rate: 3e-5
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_epsilon: 1e-8
    weight_decay: 0.1
    deepspeed_config: "./ds_config_coslr.json"
    per_device_train_batch_size: 48
    per_device_eval_batch_size: 32
    gradient_accumulation_steps: 1
    set_logging_steps: 20
    set_eval_steps: 1000
    set_save_steps: 2000
    set_compute_metrics: False
    bad_epochs_limit: 5
model:
    hidden_size: 768
    intermediate_size: 2046
    num_hidden_layers: 12
    hidden_act: "silu"
    num_attention_heads: 32
    num_key_value_heads: 4
    tokenizer_path: "./tokenizer"

