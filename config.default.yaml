name: phonelm-100M
resume: False
wandb: True
training:
    use_bf16: True
    context_size: 2048
    output_dir: "" # set to empty string to use "checkpoints-{name}" folder
    num_train_epochs: 5
    learning_rate: 3e-5
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_epsilon: 1e-8
    weight_decay: 0.1
    deepspeed_config: "./ds_config_coslr.json"
    per_device_train_batch_size: 48
    per_device_eval_batch_size: 32
    gradient_accumulation_steps: 1
    set_logging_steps: 20
    set_eval_steps: 2000
    set_save_steps: 2000
    bad_epochs_limit: 5
model:
    hidden_size: 768
    intermediate_size: 2046
    num_hidden_layers: 12
    hidden_act: "silu"
    num_attention_heads: 32
    num_key_value_heads: 4
    tie_word_embeddings: True
    tokenizer_path: "./tokenizer"
datasets:
    path: "./train_datasets"
    val_rate: 0.005

